# -*- coding: utf-8 -*-
"""adl_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N26Nujf4-xhYizNPx3_Vmurz6m8gOxTa
"""

!pip install opendatasets -q

import opendatasets as od

od.download(
    "https://www.kaggle.com/datasets/adarshrouniyar/air-pollution-image-dataset-from-india-and-nepal/data/code")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_train = pd.read_csv('/content/air-pollution-image-dataset-from-india-and-nepal/Dataset_for_AQI_Classification/Dataset_for_AQI_Classification/train_data.csv')
df_train

plt.figure(figsize=(12,6))
plt.title("Distribution of training dataset")
sns.countplot(data=df_train, x="AQI_Class")

mapp_train = {
    'a_Good': 'Good',
    'b_Moderate': 'Moderate',
    'c_Unhealthy_for_Sensitive_Groups': 'USG',
    'd_Unhealthy' : 'Unhealthy',
    'e_Very_Unhealthy' : 'Very Unhealthy',
    'f_Severe' : 'Severe'
}
df_train['Modified_AQI_Class'] = df_train['AQI_Class'].map(mapp_train)
plt.figure(figsize=(12,6))
plt.title("Distribution of training dataset")
sns.countplot(data=df_train, x="Modified_AQI_Class", palette='rocket')

df_val = pd.read_csv('/content/air-pollution-image-dataset-from-india-and-nepal/Dataset_for_AQI_Classification/Dataset_for_AQI_Classification/val_data.csv')
df_val

mapp_val = {
    'a_Good': 'Good',
    'b_Moderate': 'Moderate',
    'c_Unhealthy_for_Sensitive_Groups': 'USG',
    'd_Unhealthy' : 'Unhealthy',
    'e_Very_Unhealthy' : 'Very Unhealthy',
    'f_Severe' : 'Severe'
}
df_val['Modified_AQI_Class'] = df_val['AQI_Class'].map(mapp_train)
plt.figure(figsize=(12,6))
plt.title("Distribution of training dataset")
sns.countplot(data=df_val, x="Modified_AQI_Class", palette='rocket')

df_test = pd.read_csv('/content/air-pollution-image-dataset-from-india-and-nepal/Dataset_for_AQI_Classification/Dataset_for_AQI_Classification/testing_data.csv')
df_test

mapp_test = {
    'a_Good': 'Good',
    'b_Moderate': 'Moderate',
    'c_Unhealthy_for_Sensitive_Groups': 'USG',
    'd_Unhealthy' : 'Unhealthy',
    'e_Very_Unhealthy' : 'Very Unhealthy',
    'f_Severe' : 'Severe'
}
df_test['Modified_AQI_Class'] = df_test['AQI_Class'].map(mapp_train)
plt.figure(figsize=(12,6))
plt.title("Distribution of training dataset")
sns.countplot(data=df_test, x="Modified_AQI_Class", palette='rocket')

df_india_nepal = pd.read_csv('/content/air-pollution-image-dataset-from-india-and-nepal/Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_Nep_AQI_Dataset.csv')
df_india_nepal

image_root = '/content/air-pollution-image-dataset-from-india-and-nepal/Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_NEP'

aqi_folders = {
    'a_Good': 'Good',
    'b_Moderate': 'Moderate',
    'c_Unhealthy_for_Sensitive_Groups': 'USG',
    'd_Unhealthy': 'Unhealthy',
    'e_Very_Unhealthy': 'Very Unhealthy',
    'f_Severe': 'Severe'
}

import os

data = []
for folder_name, label in aqi_folders.items():
    folder_path = os.path.join(image_root, folder_name)
    if os.path.exists(folder_path):
        for img_file in os.listdir(folder_path):
            if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                img_path = os.path.join(folder_path, img_file)
                data.append({'filepath': img_path, 'AQI_Class': folder_name, 'Readable_Label': label})

df = pd.DataFrame(data)
print(df.head())

df['Readable_Label'] = df['AQI_Class'].map(aqi_folders)

plt.figure(figsize=(12,6))
plt.title("Distribution of training dataset - Original AQI_Class")
sns.countplot(data=df, x='AQI_Class')
plt.show()

plt.figure(figsize=(12,6))
plt.title("Distribution of training dataset - Readable_Label")
sns.countplot(data=df, x='Readable_Label', palette='rocket')
plt.show()

from sklearn.model_selection import train_test_split

train_df, temp_df = train_test_split(
    df,
    test_size=0.3,  # 30% reserved for val + test
    stratify=df['Readable_Label'],
    random_state=42
)

val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,  # half of 30% => 15% for test and 15% for val
    stratify=temp_df['Readable_Label'],
    random_state=42
)

print(f"Train set size: {len(train_df)}")
print(f"Validation set size: {len(val_df)}")
print(f"Test set size: {len(test_df)}")

# Train/Validation/Test split
train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['Readable_Label'], random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Readable_Label'], random_state=42)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Image data generators
img_size = (128, 128)
batch_size = 32
datagen = ImageDataGenerator(rescale=1./255)

train_flow = datagen.flow_from_dataframe(
    train_df, x_col='filepath', y_col='Readable_Label', target_size=img_size,
    class_mode='categorical', batch_size=batch_size, shuffle=True
)

val_flow = datagen.flow_from_dataframe(
    val_df, x_col='filepath', y_col='Readable_Label', target_size=img_size,
    class_mode='categorical', batch_size=batch_size, shuffle=False
)

test_flow = datagen.flow_from_dataframe(
    test_df, x_col='filepath', y_col='Readable_Label', target_size=img_size,
    class_mode='categorical', batch_size=batch_size, shuffle=False
)

"""Build CNN Model

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Build CNN model
num_classes = len(train_flow.class_indices)

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(img_size[0], img_size[1], 3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(train_flow, validation_data=val_flow, epochs=15)

test_loss, test_acc = model.evaluate(test_flow)
print(f'Test Accuracy: {test_acc:.2f}')

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Predict class probabilities
y_pred_probs = model.predict(test_flow)
# Pick the class with highest probability
y_pred = np.argmax(y_pred_probs, axis=1)

# Get true class indices from generator
y_true = test_flow.classes

# Get class labels list for reporting and plotting
class_labels = list(test_flow.class_indices.keys())

report = classification_report(y_true, y_pred, target_names=class_labels)
print(report)

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

#ROC Curve for multi-class
y_true_bin = label_binarize(y_true, classes=range(num_classes))
plt.figure(figsize=(12, 8))
for i in range(num_classes):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{class_labels[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-class ROC Curve')
plt.legend(loc='lower right')
plt.show()

test_gen = datagen.flow_from_dataframe(
    test_df,
    x_col='filepath',
    y_col='Readable_Label',
    target_size=img_size,
    class_mode='categorical',
    batch_size=batch_size,
    shuffle=False
)

y_pred_probs = model.predict(test_gen)

#Visualize sample predictions
import random
plt.figure(figsize=(12, 12))
indices = random.sample(range(len(test_gen.filenames)), 9)
for i, idx in enumerate(indices):
    img_path = test_gen.filepaths[idx]
    img = plt.imread(img_path)
    actual_label = class_labels[test_gen.classes[idx]]
    pred_label = class_labels[y_pred[idx]]

    plt.subplot(3, 3, i+1)
    plt.imshow(img)
    plt.title(f'Actual: {actual_label}\nPred: {pred_label}')
    plt.axis('off')

plt.tight_layout()
plt.show()

# Plot training & validation accuracy + loss
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

"""Build ResNet50 Model"""

from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.preprocessing import image
import numpy as np
import os
import matplotlib.pyplot as plt


image_root = '/content/air-pollution-image-dataset-from-india-and-nepal/Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_NEP'

img_size = (224, 224)
batch_size = 32
num_classes = 6
class_labels = ['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Severe']

#use keras library for converting images into numpy array

datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.3)
train_gen = datagen.flow_from_directory(
    image_root,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_gen = datagen.flow_from_directory(
    image_root,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(*img_size, 3))
base_model.trainable = False  # Freeze base

x = GlobalAveragePooling2D()(base_model.output)
output = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=output)

from tensorflow.keras.optimizers import Adam
model.compile(optimizer=Adam(learning_rate=0.5), loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(train_gen, validation_data=val_gen, epochs=10)

import matplotlib.pyplot as plt

# 1) Plot training & validation loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

# 2) Plot training & validation accuracy (or 'accuracy'/'val_accuracy')
plt.figure(figsize=(8, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.grid(True)
plt.show()

weights_filename = 'resnet_aqi.weights.h5'

model.save_weights(weights_filename)

model.load_weights(weights_filename)

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(*img_size, 3))
base_model.trainable = False
x = GlobalAveragePooling2D()(base_model.output)
output = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
model.load_weights(weights_filename)
print("Loaded model weights for prediction.")

pollution_percentage_map = {
    'Good': 10,
    'Moderate': 30,
    'USG': 50,
    'Unhealthy': 70,
    'Very Unhealthy': 90,
    'Severe': 100
}

def predict_and_show(model, image_root, class_labels, num_images=20):
    image_paths = []
    for root, dirs, files in os.walk(image_root):
        for f in files:
            if f.lower().endswith(('.jpg', '.jpeg', '.png')):
                image_paths.append(os.path.join(root, f))

    if len(image_paths) == 0:
        print(f"No images found in the folder: {image_root}")
        return

    sample_images = random.sample(image_paths, min(num_images, len(image_paths)))
    plt.figure(figsize=(20, 20))

    for i, img_path in enumerate(sample_images):
        img = keras_image.load_img(img_path, target_size=(224, 224))
        x = keras_image.img_to_array(img)
        x = np.expand_dims(x, axis=0)
        x = preprocess_input(x)

        preds = model.predict(x, verbose=0)
        if preds is None or len(preds) == 0:
            pred_label = "Unknown"
            pollution_pct = 0
        else:
            pred_idx = np.argmax(preds, axis=1)[0]
            pred_label = class_labels[pred_idx]
            pollution_pct = pollution_percentage_map.get(pred_label, 0)

        plt.subplot(5, 4, i + 1)
        plt.imshow(img)
        plt.title(f'Pollution: {pollution_pct}%\nClass: {pred_label}', fontsize=20, color='black')
        plt.axis('off')

    plt.tight_layout()
    plt.show()

from tensorflow.keras.preprocessing import image as keras_image
predict_and_show(model, image_root, class_labels)

"""**Build GoogleNet(Inception V3) Model**"""

from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.preprocessing import image
import numpy as np
import matplotlib.pyplot as plt

# Define constants
img_size = (299, 299)  # InceptionV3 requires 299x299
batch_size = 32
num_classes = 6
class_labels = ['Good', 'Moderate', 'USG', 'Unhealthy', 'Very Unhealthy', 'Severe']

# Data generator with preprocessing
datagen = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.3)

train_gen = datagen.flow_from_directory(
    image_root,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_gen = datagen.flow_from_directory(
    image_root,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# Load InceptionV3 base model
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(*img_size, 3))
base_model.trainable = False  # Freeze base

x = GlobalAveragePooling2D()(base_model.output)
output = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=output)

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(train_gen, validation_data=val_gen, epochs=10)

# Plot training & validation loss and accuracy
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

val_gen.reset()
inc_probs = model.predict(val_gen)
inc_pred = np.argmax(inc_probs, axis=1)
inc_true = val_gen.classes
inc_class_labels = list(val_gen.class_indices.keys())

print("InceptionV3 Classification Report:")
print(classification_report(inc_true, inc_pred, target_names=inc_class_labels))

cm = confusion_matrix(inc_true, inc_pred)
plt.figure(figsize=(8,8))
sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=inc_class_labels,
            yticklabels=inc_class_labels,
            cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('InceptionV3 Confusion Matrix')
plt.show()

y_true_bin_inc = label_binarize(inc_true, classes=range(num_classes))
plt.figure(figsize=(10,8))
for i in range(num_classes):
    fpr, tpr, _ = roc_curve(y_true_bin_inc[:, i], inc_probs[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{inc_class_labels[i]} (AUC={roc_auc:.2f})')

plt.plot([0,1],[0,1],'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('InceptionV3 Multi‑class ROC')
plt.legend(loc='lower right')
plt.show()

inc_weights_file = 'inceptionv3_aqi.weights.h5'
# Assuming 'model' variable currently holds the InceptionV3 model
model.save_weights(inc_weights_file)

def predict_and_show_inception(model, gen, class_labels, num_images=12, img_size=(299, 299), preprocess_func=preprocess_input):
    indices = random.sample(range(len(gen.filenames)), num_images)
    plt.figure(figsize=(16,16))
    for i, idx in enumerate(indices):
        img_path = gen.filepaths[idx]
        img_raw = plt.imread(img_path)
        actual_label = class_labels[gen.classes[idx]]

        img = keras_image.load_img(img_path, target_size=img_size)
        x_arr = keras_image.img_to_array(img)
        x_arr = np.expand_dims(x_arr, axis=0)
        x_arr = preprocess_func(x_arr)

        preds = model.predict(x_arr, verbose=0)
        pred_idx = np.argmax(preds, axis=1)[0]
        pred_label = class_labels[pred_idx]
        pollution_pct = pollution_percentage_map.get(pred_label, 0)

        plt.subplot(4, 3, i+1)
        plt.imshow(img_raw)
        plt.title(f'Pollution: {pollution_pct}%\nActual: {actual_label}\nPred: {pred_label}',
                  fontsize=10)
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# Call the function with correct variables for InceptionV3
predict_and_show_inception(model, val_gen, class_labels, img_size=img_size, preprocess_func=preprocess_input)

"""**Build VGG-16 Model**"""

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint


TRAIN_DIR = globals().get('image_root', None)
IMG_SIZE   = tuple(globals().get('img_size', (224,224)))
BATCH_SIZE = int(globals().get('batch_size', 32))
class_labels = globals().get('class_labels', None)

if TRAIN_DIR is None:
    raise RuntimeError("image_root not found. Please set image_root in your notebook to dataset directory.")

if class_labels is None:

    class_labels = sorted([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))])
    print("Inferred class_labels:", class_labels)

NUM_CLASSES = len(class_labels)
OUTPUT_DIR = "./vgg_outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ---------- Data generators ----------
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    width_shift_range=0.12,
    height_shift_range=0.12,
    shear_range=0.12,
    zoom_range=0.12,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_gen = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_gen = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# ---------- Callbacks & hyperparams ----------
def make_callbacks(name):
    return [
        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),
        ModelCheckpoint(os.path.join(OUTPUT_DIR, f"{name}_best.h5"), monitor='val_loss', save_best_only=True, verbose=1)
    ]

WEIGHT_DECAY = 1e-4
LABEL_SMOOTHING = 0.1

# ---------- Build model ----------
def build_vgg16(input_shape=(224,224,3), num_classes=NUM_CLASSES, weight_decay=WEIGHT_DECAY):
    base = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)
    base.trainable = False
    x = base.output
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    out = layers.Dense(num_classes, activation='softmax')(x)
    model = models.Model(inputs=base.input, outputs=out, name='vgg16_transfer_reg')
    return model

# ---------- Train functions ----------
def train_vgg(epochs_head=8, epochs_finetune=20, base_lr=1e-4):
    model = build_vgg16(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
    loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_lr), loss=loss_fn, metrics=['accuracy'])
    model.summary()
    cbs = make_callbacks('vgg16')
    print("Training VGG16 head (base frozen)...")
    history_head = model.fit(train_gen, validation_data=val_gen, epochs=epochs_head, callbacks=cbs)
    # fine-tune
    print("Unfreezing last layers and fine-tuning...")
    # Unfreeze last N conv layers of base
    base = None
    for layer in model.layers:
        if isinstance(layer, tf.keras.applications.vgg16.VGG16):
            base = layer
            break
    if base is None and len(model.layers) > 1:
        base = model.layers[1]
    if base is not None:
        base.trainable = True
        N = 8
        for l in base.layers[:-N]:
            l.trainable = False
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_lr/10), loss=loss_fn, metrics=['accuracy'])
    history_ft = model.fit(train_gen, validation_data=val_gen, epochs=epochs_finetune, callbacks=cbs)
    model.save(os.path.join(OUTPUT_DIR, "vgg16_final.h5"))
    print("Saved VGG16:", os.path.join(OUTPUT_DIR, "vgg16_final.h5"))
    return model, history_head, history_ft

# ---------- Plotting ----------
def _combine_hist(hist_list):
    if hist_list is None: return None
    if not isinstance(hist_list, (list,tuple)): hist_list=[hist_list]
    out = {'accuracy':[], 'val_accuracy':[], 'loss':[], 'val_loss':[]}
    for h in hist_list:
        if h is None: continue
        hd = h.history if hasattr(h,'history') else h
        for k in out.keys():
            if k in hd: out[k].extend(hd[k])
    return out

def plot_history_vgg(histories, title_prefix="VGG16"):
    hist = _combine_hist(histories)
    if hist is None:
        print("No history")
        return
    epochs = range(1, len(hist['accuracy'])+1)
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(epochs, hist['accuracy'], label='train_acc')
    plt.plot(epochs, hist['val_accuracy'], label='val_acc')
    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title(title_prefix+' Accuracy'); plt.legend()
    plt.subplot(1,2,2)
    plt.plot(epochs, hist['loss'], label='train_loss')
    plt.plot(epochs, hist['val_loss'], label='val_loss')
    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(title_prefix+' Loss'); plt.legend()
    plt.tight_layout(); plt.show()

# ---------- Prediction utility ----------
def _preprocess_image(path, target_size):
    pil = load_img(path, target_size=target_size)
    arr = img_to_array(pil).astype('float32') / 255.0
    arr = np.expand_dims(arr, 0)
    return pil, arr

def predict_vgg(model_or_path, image_path, top_k=3):
    if isinstance(model_or_path, str):
        model = tf.keras.models.load_model(model_or_path)
    else:
        model = model_or_path
    pil, x = _preprocess_image(image_path, IMG_SIZE)
    preds = model.predict(x)[0]
    probs = preds / np.sum(preds)
    idxs = np.argsort(probs)[::-1]
    top_idx = int(idxs[0]); top_prob = float(probs[top_idx]); top_label = class_labels[top_idx]
    plt.figure(figsize=(6,4)); plt.imshow(pil); plt.axis('off')
    plt.title(f"Pred: {top_label} — Pollution: {top_prob*100:.2f}%"); plt.show()
    print("Top predictions:")
    for i in range(min(top_k, len(idxs))):
        j = int(idxs[i]); lbl = class_labels[j]; print(f"{i+1}. {lbl}: {probs[j]*100:.2f}%")
    # bar chart
    plt.figure(figsize=(max(6, NUM_CLASSES*0.6), 4))
    ys = probs*100; xs = class_labels
    bars = plt.bar(range(len(ys)), ys)
    plt.xticks(range(len(ys)), xs, rotation=45, ha='right'); plt.ylabel('Probability (%)'); plt.title('Probabilities')
    for b in bars:
        h=b.get_height(); plt.text(b.get_x()+b.get_width()/2, h+0.5, f"{h:.1f}%", ha='center', va='bottom', fontsize=8)
    plt.tight_layout(); plt.show()
    return probs, top_idx, top_label, top_prob

"""**Build AlexNet Model**"""

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint


TRAIN_DIR = globals().get('image_root', None)
BATCH_SIZE = int(globals().get('batch_size', 32))
class_labels = globals().get('class_labels', None)

if TRAIN_DIR is None:
    raise RuntimeError("image_root not found. Please set image_root in your notebook to dataset directory.")

if class_labels is None:

    class_labels = sorted([d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))])
    print("Inferred class_labels:", class_labels)

NUM_CLASSES = len(class_labels)
ALEX_SIZE = (227,227)
OUTPUT_DIR = "./alex_outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ---------- Data generators ----------
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    width_shift_range=0.12,
    height_shift_range=0.12,
    shear_range=0.12,
    zoom_range=0.12,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_gen = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=ALEX_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)
val_gen = datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=ALEX_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=False
)

# ---------- Callbacks & hyperparams ----------
def make_callbacks(name):
    return [
        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),
        ModelCheckpoint(os.path.join(OUTPUT_DIR, f"{name}_best.h5"), monitor='val_loss', save_best_only=True, verbose=1)
    ]

WEIGHT_DECAY = 1e-4
LABEL_SMOOTHING = 0.1

# ---------- Build AlexNet ----------
def build_alexnet(input_shape=(227,227,3), num_classes=NUM_CLASSES, weight_decay=WEIGHT_DECAY):
    i = layers.Input(shape=input_shape)
    x = layers.Conv2D(96, (11,11), strides=4, activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(i)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((3,3), strides=2)(x)
    x = layers.Conv2D(256, (5,5), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((3,3), strides=2)(x)
    x = layers.Conv2D(384, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(384, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(256, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((3,3), strides=2)(x)
    x = layers.Flatten()(x)
    x = layers.Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(weight_decay))(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    out = layers.Dense(num_classes, activation='softmax')(x)
    model = models.Model(inputs=i, outputs=out, name='alexnet_like_reg')
    return model

# ---------- Train function ----------
def train_alex(epochs=30, lr=1e-2):
    model = build_alexnet(input_shape=(ALEX_SIZE[0], ALEX_SIZE[1], 3))
    loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)
    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9), loss=loss_fn, metrics=['accuracy'])
    model.summary()
    cbs = make_callbacks('alexnet')
    history = model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=cbs)
    model.save(os.path.join(OUTPUT_DIR, "alexnet_final.h5"))
    print("Saved AlexNet:", os.path.join(OUTPUT_DIR, "alexnet_final.h5"))
    return model, history

# ---------- Plotting ----------
def plot_history_alex(history, title_prefix="AlexNet"):
    if history is None:
        print("No history")
        return
    hd = history.history if hasattr(history,'history') else history
    epochs = range(1, len(hd['accuracy'])+1)
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(epochs, hd['accuracy'], label='train_acc')
    plt.plot(epochs, hd['val_accuracy'], label='val_acc')
    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title(title_prefix + ' Accuracy'); plt.legend()
    plt.subplot(1,2,2)
    plt.plot(epochs, hd['loss'], label='train_loss')
    plt.plot(epochs, hd['val_loss'], label='val_loss')
    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title(title_prefix + ' Loss'); plt.legend()
    plt.tight_layout(); plt.show()

# ---------- Prediction ----------
def _preprocess_image(path, target_size):
    pil = load_img(path, target_size=target_size)
    arr = img_to_array(pil).astype('float32') / 255.0
    arr = np.expand_dims(arr, 0)
    return pil, arr

def predict_alex(model_or_path, image_path, top_k=3):
    if isinstance(model_or_path, str):
        model = tf.keras.models.load_model(model_or_path)
    else:
        model = model_or_path
    pil, x = _preprocess_image(image_path, ALEX_SIZE)
    preds = model.predict(x)[0]
    probs = preds / np.sum(preds)
    idxs = np.argsort(probs)[::-1]
    top_idx = int(idxs[0]); top_prob = float(probs[top_idx]); top_label = class_labels[top_idx]
    plt.figure(figsize=(6,4)); plt.imshow(pil); plt.axis('off')
    plt.title(f"Pred: {top_label} — Pollution: {top_prob*100:.2f}%"); plt.show()
    print("Top predictions:")
    for i in range(min(top_k, len(idxs))):
        j = int(idxs[i]); lbl = class_labels[j]; print(f"{i+1}. {lbl}: {probs[j]*100:.2f}%")
    # bar chart
    plt.figure(figsize=(max(6, NUM_CLASSES*0.6), 4))
    ys = probs*100; xs = class_labels
    bars = plt.bar(range(len(ys)), ys)
    plt.xticks(range(len(ys)), xs, rotation=45, ha='right'); plt.ylabel('Probability (%)'); plt.title('Probabilities')
    for b in bars:
        h=b.get_height(); plt.text(b.get_x()+b.get_width()/2, h+0.5, f"{h:.1f}%", ha='center', va='bottom', fontsize=8)
    plt.tight_layout(); plt.show()
    return probs, top_idx, top_label, top_prob